# 100 Days Of Code - Log

### Day 1: March 6, 2020

**Today's Progress**: Finished "Data Aggregation" from the "Data Cleaning and Analysis" course on Dataquest.

**Thoughts:** I'm excited to start the challenge. I still struggle to understand how to apply acquired knowledge in real-life problems so I think I need to start some project (but have now ideas where to begin). Finally used to read the docs!

### Day 2: March 7, 2020

**Today's Progress**: Finished "Combining Data With Pandas" from the "Data Cleaning and Analysis" course on Dataquest.

**Thoughts:** I understood almost everything, but was quite tired today, so I need to refresh my memory tomorrow. I'd like to explore some real-life data with the methods, I've learnt today.


### Day 3: March 8, 2020

**Today's Progress**: Finished "Transforming Data With Pandas" from the "Data Cleaning and Analysis" course on Dataquest. Also learn the pivot function by experimenting with the arguments, it wasn't that diffucult as it'd seemed initially.

**Thoughts:** I was happy to understand how pivot tables are powerful, I'm also very proud of myself for understanding how to use the pivot function. Have to apply this knowledge to some other dataset to practice a bit. 


### Day 4: March 9, 2020

**Today's Progress**: Finished "Working With Strings In Pandas" from the "Data Cleaning and Analysis" course on Dataquest. Regex are quite difficult to undestand. 

**Thoughts:** I need to practice much more and try to understand what the docs say! Also have to read more carefully what I have to do in the exercise. I'm very happy that I can stay on track with the challegne, it's actually encouraging to write my thoughts on Twitter and Github.


### Day 5: March 10, 2020

**Today's Progress**: I'm on half way of "Working With Missing And Duplicate Data" from the "Data Cleaning and Analysis" course on Dataquest.

**Thoughts:** I had to split my 1 hour in two parts today. The exercises weren't diffucult but I would like to figure out another method to fill missing values from another dataset.


### Day 6: March 11, 2020

**Today's Progress**: I did some practice exercises on Dataquest.

**Thoughts:** I had to revise some base concepts, because I studied them some months ago, so it was very useful to get to basics.

### Day 7: March 12, 2020

**Today's Progress**: I continued with my Facebook ADs project. Today I cleaned a bit the code, but got some problems with Google API access. Ready to resolve everything tomorrow. 

**Thoughts:** I'm determined to finish the project in the next few days. Finally I will have all my data automatically uploaded on Google Sheets.

### Day 8: March 14, 2020

**Today's Progress**: I started to analyse the internship dataset.

**Thoughts:** It's incredible how things I studied turn to be useful! I had to apply a lot of cleaning and exploratory techniques. I will continue to learn much more.


### Day 9: March 15, 2020

**Today's Progress**: I continued to explore the dataset. Got some problems while transforming the coordinates columns in float values, need more investigaiton. Also have to understand better Geopandas.

**Thoughts:** The more I explore the dataset, the more I learn. Working on a project is actually very useful. But I got distracted by some tutorials that I didn't need at the moment. 



### Day 10: March 16, 2020

**Today's Progress**: I continued to explore the dataset. I still can't convert the coordinates in an appropriate format.

**Thoughts:** I have to ask on StackOverflow how to converts the coordinate to floats!


### Day 11: March 17, 2020

**Today's Progress**: I decided to revise and finish "Working With Missing And Duplicate Data" on Dataquest.

**Thoughts:** I start to learn a lot of in-built pandas methods to accomplish the word, I'd done manually before. It feels exciting!

### Day 12: March 18, 2020

**Today's Progress**: I finished "Working With Missing And Duplicate Data" on Dataquest and actually restarted my internship project with new instruments I have now.

**Thoughts:** I'm very excited about data analysis. I'm ready to tackle every problem I will have and to learn even more data analysis tecniques!


### Day 13: March 19, 2020

**Today's Progress**: I continue my internship project. I got a lot of problems while cleaning the dataset (especially converting numeric strings to floats), so tomorrow I will have to start over.

**Thoughts:** That's was quite difficult, I cannot undestand where I'm making a mistake, so tomorrow I'm ready to find it out. 


### Day 14: March 20, 2020

**Today's Progress**: Today I continued to work with the internship dataset, trying to convert coordinates in float numbers, but didn't succeed for God knows which reason.

**Thoughts:** That's tough, the code that did work some days ago today doesn't work at all. Have to figure it out. 


### Day 15: March 21, 2020

**Today's Progress**: Today I finally was able to convert coordinate in float numbers. The problem was with the  ```regex=False ``` in  ```replace()``` function. Then I converted the coordinates in Point of geopandas and plotted all coordinates on Asia-Europe map.

**Thoughts:** It was a great pleasure after days of trying to finally did what I was expecting!


### Day 16: March 22, 2020

**Today's Progress**: Today I converted all coordinates in ```shapely.Point``` objects and plotted them on an Asia-Europe map. Also I calculated an arbitrary distance from a point to each of coordinates.   

**Thoughts:** Days of work have finally got me to a satisfying result. Now it's time to further analyse the dataset and try to find any patterns in human mighrations. 

### Day 17: March 23, 2020

**Today's Progress**: Today I tried to create a plot that puts in relation average age of human samples and distance from an arbitrary point on the map. Found out that there's no analogue of ```geom_smooth()``` from R in Python. So tomorrow I will download ggplot for Python.

**Thoughts:** More I dive into coding more I understand how much more I have to learn and understand. But I'm excited to continue my path!

### Day 18: March 24, 2020

**Today's Progress**: I found out that ggplot is only for Python 3.6, but I also discovered that I can do a lot of stuff I want with Seaborn. I also read some interesting tutorials on how to work with Excel tables.

**Thoughts:** I'd like to learn further about Seaborn, it seems to be a powerful data visualization instrument. 


### Day 19: March 25, 2020

**Today's Progress**: Today I decided to rest a bit from the internship project and commited myself to doing a small WhatsApp chats analyzer. Got some interesting insights in one of my group. I used [this article](https://towardsdatascience.com/build-your-own-whatsapp-chat-analyzer-9590acca9014) to actually copy-paste the code, but also trying to understand what it means. I was able to understand everything, even though it wasn't easy.

**Thoughts:** I have to be more independent while building my projects.


### Day 20: March 26, 2020

**Today's Progress**: Today I revised "my" project and understood it a bit better. Also I tried to use regex to extract all emojis. Somehow it worked, but I'd like to count them.

**Thoughts:** From tomorrow I'll try to write my own functions, maybe doing a guided project on Dataquest.


### Day 21: March 27, 2020

**Today's Progress**: Since due to the coronavirus, it's not clear if the internship will continue I decided to begin a guided project on Dataquest. Today I did some initial exploration of the datasets.

**Thoughts:** I'm determined to ask to the initial questions about the dataset and also to write my observations in clear way.

### Day 22: March 28, 2020

**Today's Progress**: Today I did some data exploration of exit surveys of Australian TAFE e DETE on Dataquest. These datasets present similar data, but require a lot of data cleaning. I'm ready to start on working!

**Thoughts:** I'm excited about starting a real data science project!

### Day 23: March 29, 2020

**Today's Progress**: Today I continued the data exploration and made some interesting observations. Unfortunately I wasn't attentive enough to notice some missing data in two columns and had to read it from the Dataquest tutorial.

**Thoughts:** I have to pay much more attention while exploring the data to avoid unwanted mistakes during the analysis.
